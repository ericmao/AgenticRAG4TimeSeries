# ğŸ¤– Local LLM Integration for Agentic RAG System

## Overview

This module provides local LLM (Language Model) integration for the Agentic RAG system, replacing OpenAI ChatGPT with **Google's Gemma-2B** for completely local inference.

## ğŸ¯ Key Features

### **Local Inference**
- âœ… **No API Costs**: Completely local, no OpenAI API charges
- âœ… **Privacy**: All data stays local, no external API calls
- âœ… **Offline Capability**: Works without internet connection
- âœ… **Customizable**: Easy to switch between different local models

### **Gemma-2B Integration**
- âœ… **2B Parameters**: Efficient 2-billion parameter model
- âœ… **4-bit Quantization**: Memory-efficient loading (~4GB RAM)
- âœ… **GPU/CPU Support**: Automatic device detection and fallback
- âœ… **LangChain Compatible**: Drop-in replacement for OpenAI models

## ğŸ“¦ Installation

### 1. Install Local LLM Dependencies

```bash
python setup_local.py
```

This will install:
- `accelerate>=0.20.0` - Hugging Face acceleration
- `bitsandbytes>=0.41.0` - 4-bit quantization
- `safetensors>=0.3.0` - Safe model loading
- `tokenizers>=0.13.0` - Tokenization

### 2. Test Local LLM

```bash
python local_llm.py
```

### 3. Run with Local LLM

```bash
python main_local.py
```

## ğŸš€ Usage

### Quick Start

```bash
# Setup local LLM
python setup_local.py

# Run the system with local LLM
python main_local.py
```

### Interactive Mode

Once running, you can ask questions like:

- `"Analyze potential insider threats for user USER0001"`
- `"Search for similar behavior patterns to unusual logon times"`
- `"What are the trends in user USER0002's activity?"`
- `"Compare Markov and BERT results for user USER0003"`

## ğŸ”§ Configuration

### Environment Variables

Set these in your `.env` file or environment:

```env
# Local LLM Configuration
LOCAL_LLM_MODEL=gemma-2b
LOCAL_LLM_DEVICE=auto
LOCAL_LLM_MAX_LENGTH=2048
LOCAL_LLM_TEMPERATURE=0.1
USE_4BIT_QUANTIZATION=true
```

### Model Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `LOCAL_LLM_MODEL` | `gemma-2b` | Model to use |
| `LOCAL_LLM_DEVICE` | `auto` | Device (auto/cuda/cpu) |
| `LOCAL_LLM_MAX_LENGTH` | `2048` | Maximum sequence length |
| `LOCAL_LLM_TEMPERATURE` | `0.1` | Sampling temperature |
| `USE_4BIT_QUANTIZATION` | `true` | Use 4-bit quantization |

## ğŸ“Š Performance

### Memory Usage
- **4-bit Quantization**: ~4GB RAM
- **8-bit Quantization**: ~8GB RAM
- **Full Precision**: ~16GB RAM

### Speed
- **CPU**: ~5-10 tokens/second
- **GPU**: ~20-50 tokens/second
- **Quality**: Good for reasoning and analysis

### Hardware Requirements

#### Minimum (CPU)
- **RAM**: 8GB
- **Storage**: 5GB for model
- **CPU**: Modern multi-core

#### Recommended (GPU)
- **RAM**: 16GB
- **GPU**: 8GB+ VRAM
- **Storage**: 10GB for model

## ğŸ” System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Loading  â”‚    â”‚  Feature        â”‚    â”‚  Vector Store   â”‚
â”‚   & Processing  â”‚â”€â”€â”€â–¶â”‚  Engineering    â”‚â”€â”€â”€â–¶â”‚  & Embeddings   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Local LLM      â”‚    â”‚  Markov Chain   â”‚    â”‚  BERT-based     â”‚
â”‚  (Gemma-2B)     â”‚â—€â”€â”€â”€â”‚  Anomaly        â”‚    â”‚  Anomaly        â”‚
â”‚  Agent          â”‚    â”‚  Detector       â”‚    â”‚  Detector       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Components

### 1. **Local LLM Module** (`local_llm.py`)
- **Purpose**: Local LLM wrapper with Gemma-2B support
- **Features**:
  - 4-bit quantization for memory efficiency
  - GPU/CPU automatic detection
  - LangChain compatibility
  - Error handling and fallback

### 2. **Local Agent** (`agentic_rag_agent_local.py`)
- **Purpose**: Agentic RAG agent with local LLM
- **Features**:
  - Same 4 tools as original system
  - Local LLM integration
  - Comprehensive analysis capabilities

### 3. **Main Script** (`main_local.py`)
- **Purpose**: Main orchestration with local LLM
- **Features**:
  - Complete system initialization
  - Interactive mode
  - Example analysis

### 4. **Setup Script** (`setup_local.py`)
- **Purpose**: Automated setup for local LLM
- **Features**:
  - Dependency installation
  - Environment configuration
  - System testing

## ğŸ”„ Migration from OpenAI

### Before (OpenAI)
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.1,
    openai_api_key="your_key"
)
```

### After (Local LLM)
```python
from local_llm import create_local_llm

llm = create_local_llm(
    model_type="gemma-2b",
    temperature=0.1,
    use_4bit=True
)
```

## ğŸ¯ Benefits

### **Cost Savings**
- âŒ **OpenAI**: $0.002 per 1K tokens
- âœ… **Local**: $0.00 (one-time model download)

### **Privacy**
- âŒ **OpenAI**: Data sent to external servers
- âœ… **Local**: All data stays on your machine

### **Reliability**
- âŒ **OpenAI**: Depends on internet and API availability
- âœ… **Local**: Works offline, no API limits

### **Customization**
- âŒ **OpenAI**: Limited model options
- âœ… **Local**: Can use any Hugging Face model

## ğŸš¨ Troubleshooting

### Common Issues

1. **Out of Memory**
   ```
   RuntimeError: CUDA out of memory
   ```
   **Solution**: Use 4-bit quantization or reduce batch size

2. **Model Download Failed**
   ```
   ConnectionError: Failed to download model
   ```
   **Solution**: Check internet connection or use cached model

3. **Import Errors**
   ```
   ImportError: No module named 'bitsandbytes'
   ```
   **Solution**: Run `python setup_local.py`

4. **Slow Performance**
   ```
   Very slow token generation
   ```
   **Solution**: Use GPU if available, or reduce model size

### Performance Tips

1. **Use GPU**: Install CUDA and use GPU for faster inference
2. **4-bit Quantization**: Reduces memory usage by 75%
3. **Batch Processing**: Process multiple queries together
4. **Model Caching**: Cache model to avoid re-downloading

## ğŸ“ˆ Comparison

| Feature | OpenAI GPT-3.5 | Local Gemma-2B |
|---------|----------------|----------------|
| **Cost** | $0.002/1K tokens | Free |
| **Privacy** | Data sent to OpenAI | Completely local |
| **Speed** | ~100 tokens/sec | ~20 tokens/sec |
| **Memory** | N/A | ~4GB (4-bit) |
| **Quality** | Excellent | Good |
| **Reliability** | Internet dependent | Always available |

## ğŸ‰ Getting Started

1. **Setup**:
   ```bash
   python setup_local.py
   ```

2. **Test**:
   ```bash
   python local_llm.py
   ```

3. **Run**:
   ```bash
   python main_local.py
   ```

4. **Enjoy**: Your completely local Agentic RAG system! ğŸš€

---

**ğŸ¯ The local LLM integration provides a cost-effective, private, and reliable alternative to OpenAI while maintaining all the functionality of your Agentic RAG system!**
